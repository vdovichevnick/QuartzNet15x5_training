{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тестирование проводится на NVIDIA GeForce RTX 4070 Laptop GPU 8GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1-1'></a>\n",
    "## Тестирование Обученного Quartznet15x5 \n",
    "Размер 74 MB\n",
    "\n",
    "2050 примеров:\n",
    "\n",
    "62.7 секунды\n",
    "\n",
    "0.03 секунды на 1 пример\n",
    "\n",
    "WER 27.51%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vdovichev\\anaconda3\\envs\\nemo\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[NeMo W 2024-06-12 16:14:14 transformer_bpe_models:59] Could not import NeMo NLP collection which is required for speech translation model.\n"
     ]
    }
   ],
   "source": [
    "import nemo.collections.asr as nemo_asr\n",
    "from nemo.core.classes import ModelPT\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from ruamel.yaml import YAML\n",
    "import torchaudio\n",
    "import time\n",
    "from omegaconf import DictConfig, OmegaConf, open_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-06-12 16:30:54 modelPT:161] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath:\n",
      "    - /home/jupyter/datasphere/datasets/RuDevices/RuDevicesDataset/manifestRuDevices.jsonl\n",
      "    - /home/jupyter/datasphere/datasets/STTandCustomer/OpenSTTruDatasets/asr_calls_2_val/manifestSTTcalls.jsonl\n",
      "    - /home/jupyter/datasphere/datasets/STTandCustomer/OpenSTTruDatasets/buriy_audiobooks_2_val/manifestSTTaudiobooks.jsonl\n",
      "    - /home/jupyter/datasphere/datasets/STTandCustomer/OpenSTTruDatasets/public_youtube700_val/manifestSTTyoutube.jsonl\n",
      "    sample_rate: 16000\n",
      "    labels:\n",
      "    - ' '\n",
      "    - а\n",
      "    - б\n",
      "    - в\n",
      "    - г\n",
      "    - д\n",
      "    - е\n",
      "    - ж\n",
      "    - з\n",
      "    - и\n",
      "    - й\n",
      "    - к\n",
      "    - л\n",
      "    - м\n",
      "    - н\n",
      "    - о\n",
      "    - п\n",
      "    - р\n",
      "    - с\n",
      "    - т\n",
      "    - у\n",
      "    - ф\n",
      "    - х\n",
      "    - ц\n",
      "    - ч\n",
      "    - ш\n",
      "    - щ\n",
      "    - ъ\n",
      "    - ы\n",
      "    - ь\n",
      "    - э\n",
      "    - ю\n",
      "    - я\n",
      "    batch_size: 32\n",
      "    trim_silence: false\n",
      "    max_duration: 20.0\n",
      "    min_duration: 0.1\n",
      "    num_workers: 8\n",
      "    shuffle: true\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: null\n",
      "    tarred_shard_strategy: scatter\n",
      "    parser: ru\n",
      "    \n",
      "[NeMo W 2024-06-12 16:30:54 modelPT:168] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    name: long_audio\n",
      "    manifest_filepath: /home/jupyter/datasphere/datasets/STTandCustomer/ValidationFromCustomer/manifestValidationFromCustomer.jsonl\n",
      "    sample_rate: 16000\n",
      "    labels:\n",
      "    - ' '\n",
      "    - а\n",
      "    - б\n",
      "    - в\n",
      "    - г\n",
      "    - д\n",
      "    - е\n",
      "    - ж\n",
      "    - з\n",
      "    - и\n",
      "    - й\n",
      "    - к\n",
      "    - л\n",
      "    - м\n",
      "    - н\n",
      "    - о\n",
      "    - п\n",
      "    - р\n",
      "    - с\n",
      "    - т\n",
      "    - у\n",
      "    - ф\n",
      "    - х\n",
      "    - ц\n",
      "    - ч\n",
      "    - ш\n",
      "    - щ\n",
      "    - ъ\n",
      "    - ы\n",
      "    - ь\n",
      "    - э\n",
      "    - ю\n",
      "    - я\n",
      "    batch_size: 3\n",
      "    num_workers: 8\n",
      "    shuffle: false\n",
      "    parser: ru\n",
      "    \n",
      "[NeMo W 2024-06-12 16:30:54 modelPT:174] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath: test/mcv/test_ru.jsonl\n",
      "    sample_rate: 16000\n",
      "    labels:\n",
      "    - ' '\n",
      "    - а\n",
      "    - б\n",
      "    - в\n",
      "    - г\n",
      "    - д\n",
      "    - е\n",
      "    - ж\n",
      "    - з\n",
      "    - и\n",
      "    - й\n",
      "    - к\n",
      "    - л\n",
      "    - м\n",
      "    - н\n",
      "    - о\n",
      "    - п\n",
      "    - р\n",
      "    - с\n",
      "    - т\n",
      "    - у\n",
      "    - ф\n",
      "    - х\n",
      "    - ц\n",
      "    - ч\n",
      "    - ш\n",
      "    - щ\n",
      "    - ъ\n",
      "    - ы\n",
      "    - ь\n",
      "    - э\n",
      "    - ю\n",
      "    - я\n",
      "    batch_size: 64\n",
      "    shuffle: false\n",
      "    parser: ru\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-06-12 16:30:54 features:289] PADDING: 16\n",
      "[NeMo I 2024-06-12 16:30:54 save_restore_connector:249] Model EncDecCTCModel was successfully restored from C:\\Users\\vdovichev\\Documents\\VKR\\models_and_configs\\bestQuartz1206.nemo.\n"
     ]
    }
   ],
   "source": [
    "# загружаем модель NeMo\n",
    "asr_model = ModelPT.restore_from(r\"C:\\Users\\vdovichev\\Documents\\VKR\\models_and_configs\\bestQuartz1206.nemo\", map_location = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(asr_model.device) # проверим что cuda подключилась"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-06-12 16:30:59 features:289] PADDING: 16\n"
     ]
    }
   ],
   "source": [
    "yaml = YAML(typ='safe')\n",
    "with open(r\"C:\\Users\\vdovichev\\Documents\\VKR\\models_and_configs\\bestQuartz1206_config.yaml\") as f:\n",
    "        config_model = yaml.load(f)\n",
    "preprocessor = nemo_asr.models.EncDecCTCModel.from_config_dict(config_model['preprocessor'])\n",
    "decoder = nemo_asr.metrics.wer.CTCDecoding(config_model['decoding'], vocabulary=config_model['decoder']['vocabulary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest_paths = [\n",
    "    r\"C:\\Users\\vdovichev\\Documents\\proj1-STT\\DatasetForFineTune\\RuDevicesDataset\\manifestValidationRuDevices.jsonl\",\n",
    "    r\"C:\\Users\\vdovichev\\Documents\\proj1-STT\\DatasetForFineTune\\OpenSTTruDatasets\\asr_calls_2_val\\manifestValidationSTTcalls.jsonl\",\n",
    "    r\"C:\\Users\\vdovichev\\Documents\\proj1-STT\\DatasetForFineTune\\OpenSTTruDatasets\\buriy_audiobooks_2_val\\manifestValidationSTTaudiobooks.jsonl\",\n",
    "    r\"C:\\Users\\vdovichev\\Documents\\proj1-STT\\DatasetForFineTune\\OpenSTTruDatasets\\public_youtube700_val\\manifestValidationSTTyoutube.jsonl\"\n",
    "]\n",
    "\n",
    "base_dirs = [\n",
    "    r\"C:\\Users\\vdovichev\\Documents\\proj1-STT\\DatasetForFineTune\\RuDevicesDataset\",\n",
    "    r\"C:\\Users\\vdovichev\\Documents\\proj1-STT\\DatasetForFineTune\\OpenSTTruDatasets\\asr_calls_2_val\",\n",
    "    r\"C:\\Users\\vdovichev\\Documents\\proj1-STT\\DatasetForFineTune\\OpenSTTruDatasets\\buriy_audiobooks_2_val\",\n",
    "    r\"C:\\Users\\vdovichev\\Documents\\proj1-STT\\DatasetForFineTune\\OpenSTTruDatasets\\public_youtube700_val\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vdovichev\\Documents\\proj1-STT\\DatasetForFineTune\\RuDevicesDataset\\manifestValidationRuDevices.jsonl\n",
      "C:\\Users\\vdovichev\\Documents\\proj1-STT\\DatasetForFineTune\\OpenSTTruDatasets\\asr_calls_2_val\\manifestValidationSTTcalls.jsonl\n",
      "C:\\Users\\vdovichev\\Documents\\proj1-STT\\DatasetForFineTune\\OpenSTTruDatasets\\buriy_audiobooks_2_val\\manifestValidationSTTaudiobooks.jsonl\n",
      "C:\\Users\\vdovichev\\Documents\\proj1-STT\\DatasetForFineTune\\OpenSTTruDatasets\\public_youtube700_val\\manifestValidationSTTyoutube.jsonl\n"
     ]
    }
   ],
   "source": [
    "asr_model.eval()\n",
    "asr_model.encoder.freeze()\n",
    "asr_model.decoder.freeze()\n",
    "\n",
    "num_of_audio_all = 0\n",
    "\n",
    "hypotheses = []\n",
    "y_target = []\n",
    "\n",
    "total_time = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for manifest_path, base_dir in zip(manifest_paths, base_dirs):\n",
    "    print(manifest_path)\n",
    "    with open(manifest_path, 'r', encoding='utf-8') as f:\n",
    "        manifest_data = [json.loads(line) for line in f]\n",
    "    \n",
    "    num_of_audio = len(manifest_data)\n",
    "    num_of_audio_all += num_of_audio\n",
    "\n",
    "    for audio_index in range(num_of_audio):\n",
    "        current_audio_entry = manifest_data[audio_index]\n",
    "        y_target.append(current_audio_entry.get('text', ''))\n",
    "\n",
    "        current_audio_filename = current_audio_entry.get('audio_filepath', '')\n",
    "        file_path = os.path.join(base_dir, current_audio_filename)\n",
    "        #print(file_path)\n",
    "\n",
    "        waveform, sample_rate = torchaudio.load(file_path)\n",
    "        processed_signal, processed_signal_len = preprocessor(input_signal=waveform, length=torch.tensor([len(waveform[0])]))\n",
    "\n",
    "        processed_signal = processed_signal.to('cuda')\n",
    "        processed_signal_len = processed_signal_len.to('cuda')\n",
    "\n",
    "        encoder_output = asr_model.encoder(audio_signal=processed_signal, length=processed_signal_len)\n",
    "        logits = asr_model.decoder(encoder_output=encoder_output[0])\n",
    "        current_hypotheses, _ = decoder.ctc_decoder_predictions_tensor(logits, decoder_lengths=processed_signal_len)\n",
    "        hypotheses.extend(current_hypotheses)\n",
    "\n",
    "total_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число аудио: 2050\n",
      "Общее время: 62.71 секунд\n",
      "Среднее время на одно аудио: 0.03 секунд\n"
     ]
    }
   ],
   "source": [
    "print(f\"Число аудио: {num_of_audio_all}\")\n",
    "print(f\"Общее время: {total_time:.2f} секунд\")\n",
    "print(f\"Среднее время на одно аудио: {total_time/num_of_audio_all:.2f} секунд\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Error Rate (WER): 27.51%\n"
     ]
    }
   ],
   "source": [
    "# from jiwer import wer не подходит, т.к. он не работает, если в гипотезе есть пустые (\"\") аудио\n",
    "print(f\"Word Error Rate (WER): {nemo_asr.metrics.wer.word_error_rate(hypotheses, y_target) * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nemo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
