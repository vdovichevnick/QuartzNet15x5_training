{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тестирование проводится на NVIDIA GeForce RTX 4070 Laptop GPU 8GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1-1'></a>\n",
    "## Тестирование Whisper-Base\n",
    "Размер 290 MB\n",
    "\n",
    "2050 примеров:\n",
    "\n",
    "856.9 секунд\n",
    "\n",
    "0.4 секунды на 1 пример\n",
    "\n",
    "WER 49.35%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vdovichev\\anaconda3\\envs\\nemo\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[NeMo W 2024-06-12 12:13:31 transformer_bpe_models:59] Could not import NeMo NLP collection which is required for speech translation model.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from ruamel.yaml import YAML\n",
    "import torchaudio\n",
    "import time\n",
    "from omegaconf import DictConfig, OmegaConf, open_dict\n",
    "import whisper\n",
    "import nemo.collections.asr as nemo_asr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU Name: NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "CUDA Version: 11.8\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Загрузите модель и процессор\n",
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "# Перенесите модель на GPU, если CUDA доступен\n",
    "model.to(device)\n",
    "\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# Дополнительная информация о GPU, если доступно\n",
    "if device.type == 'cuda':\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"CUDA Version:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest_paths = [\n",
    "    r\"C:\\Users\\vdovichev\\Documents\\proj1-STT\\DatasetForFineTune\\RuDevicesDataset\\manifestValidationRuDevices.jsonl\",\n",
    "    r\"C:\\Users\\vdovichev\\Documents\\proj1-STT\\DatasetForFineTune\\OpenSTTruDatasets\\asr_calls_2_val\\manifestValidationSTTcalls.jsonl\",\n",
    "    r\"C:\\Users\\vdovichev\\Documents\\proj1-STT\\DatasetForFineTune\\OpenSTTruDatasets\\buriy_audiobooks_2_val\\manifestValidationSTTaudiobooks.jsonl\",\n",
    "    r\"C:\\Users\\vdovichev\\Documents\\proj1-STT\\DatasetForFineTune\\OpenSTTruDatasets\\public_youtube700_val\\manifestValidationSTTyoutube.jsonl\"\n",
    "]\n",
    "\n",
    "base_dirs = [\n",
    "    r\"C:\\Users\\vdovichev\\Documents\\proj1-STT\\DatasetForFineTune\\RuDevicesDataset\",\n",
    "    r\"C:\\Users\\vdovichev\\Documents\\proj1-STT\\DatasetForFineTune\\OpenSTTruDatasets\\asr_calls_2_val\",\n",
    "    r\"C:\\Users\\vdovichev\\Documents\\proj1-STT\\DatasetForFineTune\\OpenSTTruDatasets\\buriy_audiobooks_2_val\",\n",
    "    r\"C:\\Users\\vdovichev\\Documents\\proj1-STT\\DatasetForFineTune\\OpenSTTruDatasets\\public_youtube700_val\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# необходимо выполнить для соотвествия датасету\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_string(input_str):\n",
    "    # Привести строку к нижнему регистру\n",
    "    lower_str = input_str.lower()\n",
    "    \n",
    "    # Удалить все знаки препинания\n",
    "    no_punctuation_str = re.sub(r'[^\\w\\s]', '', lower_str)\n",
    "    \n",
    "    # Удалить пробелы в начале и в конце строки\n",
    "    cleaned_str = no_punctuation_str.strip()\n",
    "    \n",
    "    return cleaned_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vdovichev\\Documents\\proj1-STT\\DatasetForFineTune\\RuDevicesDataset\\manifestValidationRuDevices.jsonl\n",
      "C:\\Users\\vdovichev\\Documents\\proj1-STT\\DatasetForFineTune\\OpenSTTruDatasets\\asr_calls_2_val\\manifestValidationSTTcalls.jsonl\n",
      "C:\\Users\\vdovichev\\Documents\\proj1-STT\\DatasetForFineTune\\OpenSTTruDatasets\\buriy_audiobooks_2_val\\manifestValidationSTTaudiobooks.jsonl\n",
      "C:\\Users\\vdovichev\\Documents\\proj1-STT\\DatasetForFineTune\\OpenSTTruDatasets\\public_youtube700_val\\manifestValidationSTTyoutube.jsonl\n"
     ]
    }
   ],
   "source": [
    "num_of_audio_all = 0\n",
    "\n",
    "hypotheses = []\n",
    "y_target = []\n",
    "\n",
    "total_time = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for manifest_path, base_dir in zip(manifest_paths, base_dirs):\n",
    "    print(manifest_path)\n",
    "    with open(manifest_path, 'r', encoding='utf-8') as f:\n",
    "        manifest_data = [json.loads(line) for line in f]\n",
    "    \n",
    "    num_of_audio = len(manifest_data)\n",
    "    num_of_audio_all += num_of_audio\n",
    "\n",
    "    for audio_index in range(num_of_audio):\n",
    "        current_audio_entry = manifest_data[audio_index]\n",
    "        y_target.append(current_audio_entry.get('text', ''))\n",
    "\n",
    "        current_audio_filename = current_audio_entry.get('audio_filepath', '')\n",
    "        file_path = os.path.join(base_dir, current_audio_filename)\n",
    "        #print(file_path)\n",
    "\n",
    "        current_hypotheses = model.transcribe(file_path)[\"text\"]\n",
    "        hypotheses.append(clean_string(current_hypotheses))\n",
    "\n",
    "total_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число аудио: 2050\n",
      "Общее время: 856.91 секунд\n",
      "Среднее время на одно аудио: 0.42 секунд\n"
     ]
    }
   ],
   "source": [
    "print(f\"Число аудио: {num_of_audio_all}\")\n",
    "print(f\"Общее время: {total_time:.2f} секунд\")\n",
    "print(f\"Среднее время на одно аудио: {total_time/num_of_audio_all:.2f} секунд\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Error Rate (WER): 49.35%\n"
     ]
    }
   ],
   "source": [
    "# from jiwer import wer не подходит, т.к. он не работает, если в гипотезе есть пустые (\"\") аудио\n",
    "print(f\"Word Error Rate (WER): {nemo_asr.metrics.wer.word_error_rate(hypotheses, y_target) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1-2'></a>\n",
    "## Тестирование Quartznet15x5 Nvidia\n",
    "Размер 70 MB\n",
    "\n",
    "2050 примеров:\n",
    "\n",
    "62.4 секунды\n",
    "\n",
    "0.03 секунды на 1 пример\n",
    "\n",
    "WER 72.12%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vdovichev\\anaconda3\\envs\\nemo\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[NeMo W 2024-06-12 16:05:27 transformer_bpe_models:59] Could not import NeMo NLP collection which is required for speech translation model.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from ruamel.yaml import YAML\n",
    "import torchaudio\n",
    "import time\n",
    "from omegaconf import DictConfig, OmegaConf, open_dict\n",
    "import nemo\n",
    "import nemo.collections.asr as nemo_asr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-06-12 16:50:57 cloud:58] Found existing object C:\\Users\\vdovichev\\.cache\\torch\\NeMo\\NeMo_1.21.0\\stt_ru_quartznet15x5\\92506570b7206ea395e295b3fbbf07e3\\stt_ru_quartznet15x5.nemo.\n",
      "[NeMo I 2024-06-12 16:50:57 cloud:64] Re-using file from: C:\\Users\\vdovichev\\.cache\\torch\\NeMo\\NeMo_1.21.0\\stt_ru_quartznet15x5\\92506570b7206ea395e295b3fbbf07e3\\stt_ru_quartznet15x5.nemo\n",
      "[NeMo I 2024-06-12 16:50:57 common:913] Instantiating model from pre-trained checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-06-12 16:50:57 modelPT:161] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: /raid/noneval.json\n",
      "    sample_rate: 16000\n",
      "    labels:\n",
      "    - ' '\n",
      "    - а\n",
      "    - б\n",
      "    - в\n",
      "    - г\n",
      "    - д\n",
      "    - е\n",
      "    - ё\n",
      "    - ж\n",
      "    - з\n",
      "    - и\n",
      "    - й\n",
      "    - к\n",
      "    - л\n",
      "    - м\n",
      "    - н\n",
      "    - о\n",
      "    - п\n",
      "    - р\n",
      "    - с\n",
      "    - т\n",
      "    - у\n",
      "    - ф\n",
      "    - х\n",
      "    - ц\n",
      "    - ч\n",
      "    - ш\n",
      "    - щ\n",
      "    - ъ\n",
      "    - ы\n",
      "    - ь\n",
      "    - э\n",
      "    - ю\n",
      "    - я\n",
      "    batch_size: 16\n",
      "    trim_silence: true\n",
      "    max_duration: 16.7\n",
      "    shuffle: true\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: null\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    \n",
      "[NeMo W 2024-06-12 16:50:57 modelPT:168] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: /raid/dev.json\n",
      "    sample_rate: 16000\n",
      "    labels:\n",
      "    - ' '\n",
      "    - а\n",
      "    - б\n",
      "    - в\n",
      "    - г\n",
      "    - д\n",
      "    - е\n",
      "    - ё\n",
      "    - ж\n",
      "    - з\n",
      "    - и\n",
      "    - й\n",
      "    - к\n",
      "    - л\n",
      "    - м\n",
      "    - н\n",
      "    - о\n",
      "    - п\n",
      "    - р\n",
      "    - с\n",
      "    - т\n",
      "    - у\n",
      "    - ф\n",
      "    - х\n",
      "    - ц\n",
      "    - ч\n",
      "    - ш\n",
      "    - щ\n",
      "    - ъ\n",
      "    - ы\n",
      "    - ь\n",
      "    - э\n",
      "    - ю\n",
      "    - я\n",
      "    batch_size: 32\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-06-12 16:50:57 features:289] PADDING: 16\n",
      "[NeMo I 2024-06-12 16:50:58 save_restore_connector:249] Model EncDecCTCModel was successfully restored from C:\\Users\\vdovichev\\.cache\\torch\\NeMo\\NeMo_1.21.0\\stt_ru_quartznet15x5\\92506570b7206ea395e295b3fbbf07e3\\stt_ru_quartznet15x5.nemo.\n"
     ]
    }
   ],
   "source": [
    "asr_model = nemo_asr.models.EncDecCTCModel.from_pretrained(model_name=\"stt_ru_quartznet15x5\", map_location='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получение конфигурации модели\n",
    "config_model = asr_model.cfg\n",
    "\n",
    "# Сохранение конфигурации в YAML-файл на Google Диск\n",
    "config_yaml = OmegaConf.to_yaml(config_model)\n",
    "with open(r'C:\\Users\\vdovichev\\Documents\\VKR\\models_and_configs\\QuartzNet15x5_Nvidia_config.yaml', 'w') as yaml_file:\n",
    "    yaml_file.write(config_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-06-12 16:51:10 features:289] PADDING: 16\n"
     ]
    }
   ],
   "source": [
    "yaml = YAML(typ='safe')\n",
    "with open(r'C:\\Users\\vdovichev\\Documents\\VKR\\models_and_configs\\QuartzNet15x5_Nvidia_config.yaml') as f:\n",
    "        config_model = yaml.load(f)\n",
    "preprocessor = nemo_asr.models.EncDecCTCModel.from_config_dict(config_model['preprocessor'])\n",
    "decoder = nemo_asr.metrics.wer.CTCDecoding(config_model['decoding'], vocabulary=config_model['decoder']['vocabulary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(asr_model.device) # проверим что cuda подключилась"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest_paths = [\n",
    "    r\"C:\\Users\\vdovichev\\Documents\\proj1-STT\\DatasetForFineTune\\RuDevicesDataset\\manifestValidationRuDevices.jsonl\",\n",
    "    r\"C:\\Users\\vdovichev\\Documents\\proj1-STT\\DatasetForFineTune\\OpenSTTruDatasets\\asr_calls_2_val\\manifestValidationSTTcalls.jsonl\",\n",
    "    r\"C:\\Users\\vdovichev\\Documents\\proj1-STT\\DatasetForFineTune\\OpenSTTruDatasets\\buriy_audiobooks_2_val\\manifestValidationSTTaudiobooks.jsonl\",\n",
    "    r\"C:\\Users\\vdovichev\\Documents\\proj1-STT\\DatasetForFineTune\\OpenSTTruDatasets\\public_youtube700_val\\manifestValidationSTTyoutube.jsonl\"\n",
    "]\n",
    "\n",
    "base_dirs = [\n",
    "    r\"C:\\Users\\vdovichev\\Documents\\proj1-STT\\DatasetForFineTune\\RuDevicesDataset\",\n",
    "    r\"C:\\Users\\vdovichev\\Documents\\proj1-STT\\DatasetForFineTune\\OpenSTTruDatasets\\asr_calls_2_val\",\n",
    "    r\"C:\\Users\\vdovichev\\Documents\\proj1-STT\\DatasetForFineTune\\OpenSTTruDatasets\\buriy_audiobooks_2_val\",\n",
    "    r\"C:\\Users\\vdovichev\\Documents\\proj1-STT\\DatasetForFineTune\\OpenSTTruDatasets\\public_youtube700_val\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vdovichev\\Documents\\proj1-STT\\DatasetForFineTune\\RuDevicesDataset\\manifestValidationRuDevices.jsonl\n",
      "C:\\Users\\vdovichev\\Documents\\proj1-STT\\DatasetForFineTune\\OpenSTTruDatasets\\asr_calls_2_val\\manifestValidationSTTcalls.jsonl\n",
      "C:\\Users\\vdovichev\\Documents\\proj1-STT\\DatasetForFineTune\\OpenSTTruDatasets\\buriy_audiobooks_2_val\\manifestValidationSTTaudiobooks.jsonl\n",
      "C:\\Users\\vdovichev\\Documents\\proj1-STT\\DatasetForFineTune\\OpenSTTruDatasets\\public_youtube700_val\\manifestValidationSTTyoutube.jsonl\n"
     ]
    }
   ],
   "source": [
    "asr_model.eval()\n",
    "asr_model.encoder.freeze()\n",
    "asr_model.decoder.freeze()\n",
    "\n",
    "num_of_audio_all = 0\n",
    "\n",
    "hypotheses = []\n",
    "y_target = []\n",
    "\n",
    "total_time = 0\n",
    "start_time = time.time() \n",
    "\n",
    "for manifest_path, base_dir in zip(manifest_paths, base_dirs):\n",
    "    print(manifest_path)\n",
    "    with open(manifest_path, 'r', encoding='utf-8') as f:\n",
    "        manifest_data = [json.loads(line) for line in f]\n",
    "    \n",
    "    num_of_audio = len(manifest_data)\n",
    "    num_of_audio_all += num_of_audio\n",
    "\n",
    "    for audio_index in range(num_of_audio):\n",
    "        current_audio_entry = manifest_data[audio_index]\n",
    "        y_target.append(current_audio_entry.get('text', ''))\n",
    "\n",
    "        current_audio_filename = current_audio_entry.get('audio_filepath', '')\n",
    "        file_path = os.path.join(base_dir, current_audio_filename)\n",
    "        #print(file_path)\n",
    "\n",
    "        waveform, sample_rate = torchaudio.load(file_path)\n",
    "        processed_signal, processed_signal_len = preprocessor(input_signal=waveform, length=torch.tensor([len(waveform[0])]))\n",
    "\n",
    "        processed_signal = processed_signal.to('cuda')\n",
    "        processed_signal_len = processed_signal_len.to('cuda')\n",
    "\n",
    "        encoder_output = asr_model.encoder(audio_signal=processed_signal, length=processed_signal_len)\n",
    "        logits = asr_model.decoder(encoder_output=encoder_output[0])\n",
    "        current_hypotheses, _ = decoder.ctc_decoder_predictions_tensor(logits, decoder_lengths=processed_signal_len)\n",
    "        hypotheses.extend(current_hypotheses)\n",
    "\n",
    "total_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число аудио: 2050\n",
      "Общее время: 62.39 секунд\n",
      "Среднее время на одно аудио: 0.03 секунд\n"
     ]
    }
   ],
   "source": [
    "print(f\"Число аудио: {num_of_audio_all}\")\n",
    "print(f\"Общее время: {total_time:.2f} секунд\")\n",
    "print(f\"Среднее время на одно аудио: {total_time/num_of_audio_all:.2f} секунд\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Error Rate (WER): 72.12%\n"
     ]
    }
   ],
   "source": [
    "# from jiwer import wer не подходит, т.к. он не работает, если в гипотезе есть пустые (\"\") аудио\n",
    "print(f\"Word Error Rate (WER): {nemo_asr.metrics.wer.word_error_rate(hypotheses, y_target) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1-3'></a>\n",
    "## Тестирование Quartznet15x5 Golos\n",
    "Размер 70 MB\n",
    "\n",
    "2050 примеров:\n",
    "\n",
    "61.5 секунды\n",
    "\n",
    "0.03 секунды на 1 пример\n",
    "\n",
    "WER 50.47%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nemo.collections.asr as nemo_asr\n",
    "from nemo.core.classes import ModelPT\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from ruamel.yaml import YAML\n",
    "import torchaudio\n",
    "import time\n",
    "from omegaconf import DictConfig, OmegaConf, open_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-06-12 16:09:16 modelPT:161] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: train/golos_and_mcv.jsonl\n",
      "    sample_rate: 16000\n",
      "    labels:\n",
      "    - ' '\n",
      "    - а\n",
      "    - б\n",
      "    - в\n",
      "    - г\n",
      "    - д\n",
      "    - е\n",
      "    - ж\n",
      "    - з\n",
      "    - и\n",
      "    - й\n",
      "    - к\n",
      "    - л\n",
      "    - м\n",
      "    - н\n",
      "    - о\n",
      "    - п\n",
      "    - р\n",
      "    - с\n",
      "    - т\n",
      "    - у\n",
      "    - ф\n",
      "    - х\n",
      "    - ц\n",
      "    - ч\n",
      "    - ш\n",
      "    - щ\n",
      "    - ъ\n",
      "    - ы\n",
      "    - ь\n",
      "    - э\n",
      "    - ю\n",
      "    - я\n",
      "    batch_size: 64\n",
      "    trim_silence: false\n",
      "    max_duration: 20.0\n",
      "    min_duration: 0.1\n",
      "    num_workers: 20\n",
      "    shuffle: true\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: null\n",
      "    tarred_shard_strategy: scatter\n",
      "    parser: ru\n",
      "    \n",
      "[NeMo W 2024-06-12 16:09:16 modelPT:168] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath:\n",
      "    - test/mcv/dev_ru.jsonl\n",
      "    - test/mcv/test_ru.jsonl\n",
      "    - test/crowd/crowd.jsonl\n",
      "    - test/farfield/farfield.jsonl\n",
      "    sample_rate: 16000\n",
      "    labels:\n",
      "    - ' '\n",
      "    - а\n",
      "    - б\n",
      "    - в\n",
      "    - г\n",
      "    - д\n",
      "    - е\n",
      "    - ж\n",
      "    - з\n",
      "    - и\n",
      "    - й\n",
      "    - к\n",
      "    - л\n",
      "    - м\n",
      "    - н\n",
      "    - о\n",
      "    - п\n",
      "    - р\n",
      "    - с\n",
      "    - т\n",
      "    - у\n",
      "    - ф\n",
      "    - х\n",
      "    - ц\n",
      "    - ч\n",
      "    - ш\n",
      "    - щ\n",
      "    - ъ\n",
      "    - ы\n",
      "    - ь\n",
      "    - э\n",
      "    - ю\n",
      "    - я\n",
      "    batch_size: 64\n",
      "    num_workers: 20\n",
      "    shuffle: false\n",
      "    parser: ru\n",
      "    \n",
      "[NeMo W 2024-06-12 16:09:16 modelPT:174] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath: test/mcv/test_ru.jsonl\n",
      "    sample_rate: 16000\n",
      "    labels:\n",
      "    - ' '\n",
      "    - а\n",
      "    - б\n",
      "    - в\n",
      "    - г\n",
      "    - д\n",
      "    - е\n",
      "    - ж\n",
      "    - з\n",
      "    - и\n",
      "    - й\n",
      "    - к\n",
      "    - л\n",
      "    - м\n",
      "    - н\n",
      "    - о\n",
      "    - п\n",
      "    - р\n",
      "    - с\n",
      "    - т\n",
      "    - у\n",
      "    - ф\n",
      "    - х\n",
      "    - ц\n",
      "    - ч\n",
      "    - ш\n",
      "    - щ\n",
      "    - ъ\n",
      "    - ы\n",
      "    - ь\n",
      "    - э\n",
      "    - ю\n",
      "    - я\n",
      "    batch_size: 64\n",
      "    shuffle: false\n",
      "    parser: ru\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-06-12 16:09:16 features:289] PADDING: 16\n",
      "[NeMo I 2024-06-12 16:09:16 save_restore_connector:249] Model EncDecCTCModel was successfully restored from C:\\Users\\vdovichev\\Documents\\VKR\\models_and_configs\\QuartzNet15x5_golos.nemo.\n"
     ]
    }
   ],
   "source": [
    "# загружаем модель NeMo\n",
    "asr_model = ModelPT.restore_from(r\"C:\\Users\\vdovichev\\Documents\\VKR\\models_and_configs\\QuartzNet15x5_golos.nemo\", map_location = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(asr_model.device) # проверим что cuda подключилась"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получение конфигурации модели\n",
    "config_model = asr_model.cfg\n",
    "\n",
    "# Сохранение конфигурации в YAML-файл на Google Диск\n",
    "config_yaml = OmegaConf.to_yaml(config_model)\n",
    "with open(r'C:\\Users\\vdovichev\\Documents\\VKR\\models_and_configs\\QuartzNet15x5_golos_config.yaml', 'w') as yaml_file:\n",
    "    yaml_file.write(config_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-06-12 16:09:29 features:289] PADDING: 16\n"
     ]
    }
   ],
   "source": [
    "yaml = YAML(typ='safe')\n",
    "with open(r'C:\\Users\\vdovichev\\Documents\\VKR\\models_and_configs\\QuartzNet15x5_golos_config.yaml') as f:\n",
    "        config_model = yaml.load(f)\n",
    "preprocessor = nemo_asr.models.EncDecCTCModel.from_config_dict(config_model['preprocessor'])\n",
    "decoder = nemo_asr.metrics.wer.CTCDecoding(config_model['decoding'], vocabulary=config_model['decoder']['vocabulary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest_paths = [\n",
    "    r\"C:\\Users\\vdovichev\\Documents\\proj1-STT\\DatasetForFineTune\\RuDevicesDataset\\manifestValidationRuDevices.jsonl\",\n",
    "    r\"C:\\Users\\vdovichev\\Documents\\proj1-STT\\DatasetForFineTune\\OpenSTTruDatasets\\asr_calls_2_val\\manifestValidationSTTcalls.jsonl\",\n",
    "    r\"C:\\Users\\vdovichev\\Documents\\proj1-STT\\DatasetForFineTune\\OpenSTTruDatasets\\buriy_audiobooks_2_val\\manifestValidationSTTaudiobooks.jsonl\",\n",
    "    r\"C:\\Users\\vdovichev\\Documents\\proj1-STT\\DatasetForFineTune\\OpenSTTruDatasets\\public_youtube700_val\\manifestValidationSTTyoutube.jsonl\"\n",
    "]\n",
    "\n",
    "base_dirs = [\n",
    "    r\"C:\\Users\\vdovichev\\Documents\\proj1-STT\\DatasetForFineTune\\RuDevicesDataset\",\n",
    "    r\"C:\\Users\\vdovichev\\Documents\\proj1-STT\\DatasetForFineTune\\OpenSTTruDatasets\\asr_calls_2_val\",\n",
    "    r\"C:\\Users\\vdovichev\\Documents\\proj1-STT\\DatasetForFineTune\\OpenSTTruDatasets\\buriy_audiobooks_2_val\",\n",
    "    r\"C:\\Users\\vdovichev\\Documents\\proj1-STT\\DatasetForFineTune\\OpenSTTruDatasets\\public_youtube700_val\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vdovichev\\Documents\\proj1-STT\\DatasetForFineTune\\RuDevicesDataset\\manifestValidationRuDevices.jsonl\n",
      "C:\\Users\\vdovichev\\Documents\\proj1-STT\\DatasetForFineTune\\OpenSTTruDatasets\\asr_calls_2_val\\manifestValidationSTTcalls.jsonl\n",
      "C:\\Users\\vdovichev\\Documents\\proj1-STT\\DatasetForFineTune\\OpenSTTruDatasets\\buriy_audiobooks_2_val\\manifestValidationSTTaudiobooks.jsonl\n",
      "C:\\Users\\vdovichev\\Documents\\proj1-STT\\DatasetForFineTune\\OpenSTTruDatasets\\public_youtube700_val\\manifestValidationSTTyoutube.jsonl\n"
     ]
    }
   ],
   "source": [
    "asr_model.eval()\n",
    "asr_model.encoder.freeze()\n",
    "asr_model.decoder.freeze()\n",
    "\n",
    "num_of_audio_all = 0\n",
    "\n",
    "hypotheses = []\n",
    "y_target = []\n",
    "\n",
    "total_time = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for manifest_path, base_dir in zip(manifest_paths, base_dirs):\n",
    "    print(manifest_path)\n",
    "    with open(manifest_path, 'r', encoding='utf-8') as f:\n",
    "        manifest_data = [json.loads(line) for line in f]\n",
    "    \n",
    "    num_of_audio = len(manifest_data)\n",
    "    num_of_audio_all += num_of_audio\n",
    "\n",
    "    for audio_index in range(num_of_audio):\n",
    "        current_audio_entry = manifest_data[audio_index]\n",
    "        y_target.append(current_audio_entry.get('text', ''))\n",
    "\n",
    "        current_audio_filename = current_audio_entry.get('audio_filepath', '')\n",
    "        file_path = os.path.join(base_dir, current_audio_filename)\n",
    "        #print(file_path)\n",
    "\n",
    "        waveform, sample_rate = torchaudio.load(file_path)\n",
    "        processed_signal, processed_signal_len = preprocessor(input_signal=waveform, length=torch.tensor([len(waveform[0])]))\n",
    "\n",
    "        processed_signal = processed_signal.to('cuda')\n",
    "        processed_signal_len = processed_signal_len.to('cuda')\n",
    "\n",
    "        encoder_output = asr_model.encoder(audio_signal=processed_signal, length=processed_signal_len)\n",
    "        logits = asr_model.decoder(encoder_output=encoder_output[0])\n",
    "        current_hypotheses, _ = decoder.ctc_decoder_predictions_tensor(logits, decoder_lengths=processed_signal_len)\n",
    "        hypotheses.extend(current_hypotheses)\n",
    "\n",
    "total_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число аудио: 2050\n",
      "Общее время: 61.48 секунд\n",
      "Среднее время на одно аудио: 0.03 секунд\n"
     ]
    }
   ],
   "source": [
    "print(f\"Число аудио: {num_of_audio_all}\")\n",
    "print(f\"Общее время: {total_time:.2f} секунд\")\n",
    "print(f\"Среднее время на одно аудио: {total_time/num_of_audio_all:.2f} секунд\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Error Rate (WER): 50.47%\n"
     ]
    }
   ],
   "source": [
    "# from jiwer import wer не подходит, т.к. он не работает, если в гипотезе есть пустые (\"\") аудио\n",
    "print(f\"Word Error Rate (WER): {nemo_asr.metrics.wer.word_error_rate(hypotheses, y_target) * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nemo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
